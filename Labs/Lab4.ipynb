{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNr+/7aJaIAO+adQB6JS80+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashmcmn/NLE_Notes/blob/main/Labs/Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uit9l5SVurL3"
      },
      "source": [
        "# Intro\n",
        "This week you are going to be investigating linguistic regularities in pre-built word embeddings. You should have linked or downloaded the embeddings in week 1 (see Canvas), uncompressed them and made them accessible in your working directory. As before, you can load them in to python using the following code (it may take a while to run this because the embeddings file is very large (1.5G))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RbYEaoDwdYT"
      },
      "source": [
        "!gdown --id '1sQ2x3j4Yr6g6uqMmb41ulOACwGp0AJPK'\n",
        "!unzip 'lab4data.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C6JvSGJuu-X"
      },
      "source": [
        "from gensim.models import KeyedVectors \n",
        "mymodel = KeyedVectors.load_word2vec_format('lab4data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNhJVGNlu2xP"
      },
      "source": [
        "You can now query the model with calls to methods of mymodel such as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV5MOFZJuntm",
        "outputId": "786e246b-b56a-4101-8b14-08af0ac58faa"
      },
      "source": [
        "print(mymodel.similarity('man', 'woman'))\n",
        "print(mymodel.most_similar(positive=['man']))\n",
        "print(mymodel['man'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.76640123\n",
            "[('woman', 0.7664012908935547), ('boy', 0.6824870109558105), ('teenager', 0.6586930155754089), ('teenage_girl', 0.6147903800010681), ('girl', 0.5921714305877686), ('suspected_purse_snatcher', 0.5716364979743958), ('robber', 0.5585119128227234), ('Robbery_suspect', 0.5584409236907959), ('teen_ager', 0.5549196600914001), ('men', 0.5489763021469116)]\n",
            "[ 0.32617188  0.13085938  0.03466797 -0.08300781  0.08984375 -0.04125977\n",
            " -0.19824219  0.00689697  0.14355469  0.0019455   0.02880859 -0.25\n",
            " -0.08398438 -0.15136719 -0.10205078  0.04077148 -0.09765625  0.05932617\n",
            "  0.02978516 -0.10058594 -0.13085938  0.001297    0.02612305 -0.27148438\n",
            "  0.06396484 -0.19140625 -0.078125    0.25976562  0.375      -0.04541016\n",
            "  0.16210938  0.13671875 -0.06396484 -0.02062988 -0.09667969  0.25390625\n",
            "  0.24804688 -0.12695312  0.07177734  0.3203125   0.03149414 -0.03857422\n",
            "  0.21191406 -0.00811768  0.22265625 -0.13476562 -0.07617188  0.01049805\n",
            " -0.05175781  0.03808594 -0.13378906  0.125       0.0559082  -0.18261719\n",
            "  0.08154297 -0.08447266 -0.07763672 -0.04345703  0.08105469 -0.01092529\n",
            "  0.17480469  0.30664062 -0.04321289 -0.01416016  0.09082031 -0.00927734\n",
            " -0.03442383 -0.11523438  0.12451172 -0.0246582   0.08544922  0.14355469\n",
            " -0.27734375  0.03662109 -0.11035156  0.13085938 -0.01721191 -0.08056641\n",
            " -0.00708008 -0.02954102  0.30078125 -0.09033203  0.03149414 -0.18652344\n",
            " -0.11181641  0.10253906 -0.25976562 -0.02209473  0.16796875 -0.05322266\n",
            " -0.14550781 -0.01049805 -0.03039551 -0.03857422  0.11523438 -0.0062561\n",
            " -0.13964844  0.08007812  0.06103516 -0.15332031 -0.11132812 -0.14160156\n",
            "  0.19824219 -0.06933594  0.29296875 -0.16015625  0.20898438  0.00041771\n",
            "  0.01831055 -0.20214844  0.04760742  0.05810547 -0.0123291  -0.01989746\n",
            " -0.00364685 -0.0135498  -0.08251953 -0.03149414  0.00717163  0.20117188\n",
            "  0.08300781 -0.0480957  -0.26367188 -0.09667969 -0.22558594 -0.09667969\n",
            "  0.06494141 -0.02502441  0.08496094  0.03198242 -0.07568359 -0.25390625\n",
            " -0.11669922 -0.01446533 -0.16015625 -0.00701904 -0.05712891  0.02807617\n",
            " -0.09179688  0.25195312  0.24121094  0.06640625  0.12988281  0.17089844\n",
            " -0.13671875  0.1875     -0.10009766 -0.04199219 -0.12011719  0.00524902\n",
            "  0.15625    -0.203125   -0.07128906 -0.06103516  0.01635742  0.18261719\n",
            "  0.03588867 -0.04248047  0.16796875 -0.15039062 -0.16992188  0.01831055\n",
            "  0.27734375 -0.01269531 -0.0390625  -0.15429688  0.18457031 -0.07910156\n",
            "  0.09033203 -0.02709961  0.08251953  0.06738281 -0.16113281 -0.19628906\n",
            " -0.15234375 -0.04711914  0.04760742  0.05908203 -0.16894531 -0.14941406\n",
            "  0.12988281  0.04321289  0.02624512 -0.1796875  -0.19628906  0.06445312\n",
            "  0.08935547  0.1640625  -0.03808594 -0.09814453 -0.01483154  0.1875\n",
            "  0.12792969  0.22753906  0.01818848 -0.07958984 -0.11376953 -0.06933594\n",
            " -0.15527344 -0.08105469 -0.09277344 -0.11328125 -0.15136719 -0.08007812\n",
            " -0.05126953 -0.15332031  0.11669922  0.06835938  0.0324707  -0.33984375\n",
            " -0.08154297 -0.08349609  0.04003906  0.04907227 -0.24121094 -0.13476562\n",
            " -0.05932617  0.12158203 -0.34179688  0.16503906  0.06176758 -0.18164062\n",
            "  0.20117188 -0.07714844  0.1640625   0.00402832  0.30273438 -0.10009766\n",
            " -0.13671875 -0.05957031  0.0625     -0.21289062 -0.06542969  0.1796875\n",
            " -0.07763672 -0.01928711 -0.15039062 -0.00106049  0.03417969  0.03344727\n",
            "  0.19335938  0.01965332 -0.19921875 -0.10644531  0.01525879  0.00927734\n",
            "  0.01416016 -0.02392578  0.05883789  0.02368164  0.125       0.04760742\n",
            " -0.05566406  0.11572266  0.14746094  0.1015625  -0.07128906 -0.07714844\n",
            " -0.12597656  0.0291748   0.09521484 -0.12402344 -0.109375   -0.12890625\n",
            "  0.16308594  0.28320312 -0.03149414  0.12304688 -0.23242188 -0.09375\n",
            " -0.12988281  0.0135498  -0.03881836 -0.08251953  0.00897217  0.16308594\n",
            "  0.10546875 -0.13867188 -0.16503906 -0.03857422  0.10839844 -0.10498047\n",
            "  0.06396484  0.38867188 -0.05981445 -0.0612793  -0.10449219 -0.16796875\n",
            "  0.07177734  0.13964844  0.15527344 -0.03125    -0.20214844 -0.12988281\n",
            " -0.10058594 -0.06396484 -0.08349609 -0.30273438 -0.08007812  0.02099609]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxkSm-4au4zH"
      },
      "source": [
        "Mikolov et al. (2013) propose the use of an offset vector method in order to answer analogy questions. For example, if you want to find the concept X which satisfies the analogy “X is to China as London is to England”, you need to find the concept closest to the point X in the vector space:\n",
        "\n",
        "$X=vector_{China}-(vector_{England}-vector_{London})$\n",
        "\n",
        "$X=vector_{China}+vector_{London}-vector_{Engalnd}$\n",
        "\n",
        "You can do this with gensim using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYDrZkyGvk0c",
        "outputId": "a126b76e-b992-49cb-f155-99fb74976f85"
      },
      "source": [
        "mymodel.most_similar(positive=['China','London'], negative=['England'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Beijing', 0.6737731695175171),\n",
              " ('Shanghai', 0.646628737449646),\n",
              " ('Beijng', 0.5856549739837646),\n",
              " ('Hong_Kong', 0.5709935426712036),\n",
              " ('Chinese', 0.5639771223068237),\n",
              " ('Guangdong', 0.5119545459747314),\n",
              " ('Shenzhen', 0.5102902054786682),\n",
              " ('Yanqi', 0.5076327323913574),\n",
              " ('Nanjing', 0.5056864023208618),\n",
              " ('Guangzhou', 0.5043154954910278)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbJYyqAGvlbC"
      },
      "source": [
        "The file relations.json contains lists of pairs of words which satisfy some syntactic or semantic\n",
        "relation. You can load it into a dictionary using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5krLym7bvmph",
        "outputId": "847e602d-d9de-4db3-cbe3-dd4582be964c"
      },
      "source": [
        "import json\n",
        "with open('lab4data/relations.json', 'r') as fp:\n",
        "  testtuples=json.load(fp) \n",
        "  print(testtuples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'gram3-comparative': [['bad', 'worse'], ['big', 'bigger'], ['bright', 'brighter'], ['cheap', 'cheaper'], ['cold', 'colder'], ['cool', 'cooler'], ['deep', 'deeper'], ['easy', 'easier'], ['fast', 'faster'], ['good', 'better'], ['great', 'greater'], ['hard', 'harder'], ['heavy', 'heavier'], ['high', 'higher'], ['hot', 'hotter'], ['large', 'larger'], ['long', 'longer'], ['loud', 'louder'], ['low', 'lower'], ['new', 'newer'], ['old', 'older'], ['quick', 'quicker'], ['safe', 'safer'], ['sharp', 'sharper'], ['short', 'shorter'], ['simple', 'simpler'], ['slow', 'slower'], ['small', 'smaller'], ['smart', 'smarter'], ['strong', 'stronger'], ['tall', 'taller'], ['tight', 'tighter'], ['tough', 'tougher'], ['warm', 'warmer'], ['weak', 'weaker'], ['wide', 'wider'], ['young', 'younger']], 'gram8-plural': [['banana', 'bananas'], ['bird', 'birds'], ['bottle', 'bottles'], ['building', 'buildings'], ['car', 'cars'], ['cat', 'cats'], ['child', 'children'], ['cloud', 'clouds'], ['color', 'colors'], ['computer', 'computers'], ['cow', 'cows'], ['dog', 'dogs'], ['dollar', 'dollars'], ['donkey', 'donkeys'], ['dream', 'dreams'], ['eagle', 'eagles'], ['elephant', 'elephants'], ['eye', 'eyes'], ['finger', 'fingers'], ['goat', 'goats'], ['hand', 'hands'], ['horse', 'horses'], ['lion', 'lions'], ['machine', 'machines'], ['mango', 'mangoes'], ['man', 'men'], ['melon', 'melons'], ['monkey', 'monkeys'], ['mouse', 'mice'], ['onion', 'onions'], ['pear', 'pears'], ['pig', 'pigs'], ['pineapple', 'pineapples'], ['rat', 'rats'], ['road', 'roads'], ['snake', 'snakes'], ['woman', 'women']], 'capital-common-countries': [['Athens', 'Greece'], ['Baghdad', 'Iraq'], ['Bangkok', 'Thailand'], ['Beijing', 'China'], ['Berlin', 'Germany'], ['Bern', 'Switzerland'], ['Cairo', 'Egypt'], ['Canberra', 'Australia'], ['Hanoi', 'Vietnam'], ['Havana', 'Cuba'], ['Helsinki', 'Finland'], ['Islamabad', 'Pakistan'], ['Kabul', 'Afghanistan'], ['London', 'England'], ['Madrid', 'Spain'], ['Moscow', 'Russia'], ['Oslo', 'Norway'], ['Ottawa', 'Canada'], ['Paris', 'France'], ['Rome', 'Italy'], ['Stockholm', 'Sweden'], ['Tehran', 'Iran'], ['Tokyo', 'Japan']], 'city-in-state': [['Chicago', 'Illinois'], ['Houston', 'Texas'], ['Philadelphia', 'Pennsylvania'], ['Phoenix', 'Arizona'], ['Dallas', 'Texas'], ['Jacksonville', 'Florida'], ['Indianapolis', 'Indiana'], ['Austin', 'Texas'], ['Detroit', 'Michigan'], ['Memphis', 'Tennessee'], ['Boston', 'Massachusetts'], ['Seattle', 'Washington'], ['Denver', 'Colorado'], ['Baltimore', 'Maryland'], ['Nashville', 'Tennessee'], ['Louisville', 'Kentucky'], ['Milwaukee', 'Wisconsin'], ['Portland', 'Oregon'], ['Tucson', 'Arizona'], ['Fresno', 'California'], ['Sacramento', 'California'], ['Mesa', 'Arizona'], ['Atlanta', 'Georgia'], ['Omaha', 'Nebraska'], ['Miami', 'Florida'], ['Tulsa', 'Oklahoma'], ['Oakland', 'California'], ['Cleveland', 'Ohio'], ['Minneapolis', 'Minnesota'], ['Wichita', 'Kansas'], ['Arlington', 'Texas'], ['Bakersfield', 'California'], ['Tampa', 'Florida'], ['Anaheim', 'California'], ['Honolulu', 'Hawaii'], ['Pittsburgh', 'Pennsylvania'], ['Lexington', 'Kentucky'], ['Stockton', 'California'], ['Cincinnati', 'Ohio'], ['Anchorage', 'Alaska'], ['Toledo', 'Ohio'], ['Plano', 'Texas'], ['Henderson', 'Nevada'], ['Orlando', 'Florida'], ['Laredo', 'Texas'], ['Chandler', 'Arizona'], ['Madison', 'Wisconsin'], ['Lubbock', 'Texas'], ['Garland', 'Texas'], ['Glendale', 'Arizona'], ['Hialeah', 'Florida'], ['Reno', 'Nevada'], ['Scottsdale', 'Arizona'], ['Irving', 'Texas'], ['Fremont', 'California'], ['Irvine', 'California'], ['Spokane', 'Washington'], ['Modesto', 'California'], ['Shreveport', 'Louisiana'], ['Tacoma', 'Washington'], ['Oxnard', 'California'], ['Fontana', 'California'], ['Akron', 'Ohio'], ['Amarillo', 'Texas'], ['Glendale', 'California'], ['Tallahassee', 'Florida'], ['Huntsville', 'Alabama'], ['Worcester', 'Massachusetts']], 'family': [['boy', 'girl'], ['brother', 'sister'], ['brothers', 'sisters'], ['dad', 'mom'], ['father', 'mother'], ['grandfather', 'grandmother'], ['grandpa', 'grandma'], ['grandson', 'granddaughter'], ['groom', 'bride'], ['he', 'she'], ['his', 'her'], ['husband', 'wife'], ['king', 'queen'], ['man', 'woman'], ['nephew', 'niece'], ['policeman', 'policewoman'], ['prince', 'princess'], ['son', 'daughter'], ['sons', 'daughters'], ['stepbrother', 'stepsister'], ['stepfather', 'stepmother'], ['stepson', 'stepdaughter'], ['uncle', 'aunt']], 'gram2-opposite': [['acceptable', 'unacceptable'], ['aware', 'unaware'], ['certain', 'uncertain'], ['clear', 'unclear'], ['comfortable', 'uncomfortable'], ['competitive', 'uncompetitive'], ['consistent', 'inconsistent'], ['convincing', 'unconvincing'], ['convenient', 'inconvenient'], ['decided', 'undecided'], ['efficient', 'inefficient'], ['ethical', 'unethical'], ['fortunate', 'unfortunate'], ['honest', 'dishonest'], ['impressive', 'unimpressive'], ['informative', 'uninformative'], ['informed', 'uninformed'], ['known', 'unknown'], ['likely', 'unlikely'], ['logical', 'illogical'], ['pleasant', 'unpleasant'], ['possible', 'impossible'], ['possibly', 'impossibly'], ['productive', 'unproductive'], ['rational', 'irrational'], ['reasonable', 'unreasonable'], ['responsible', 'irresponsible'], ['sure', 'unsure'], ['tasteful', 'distasteful']], 'currency': [['Algeria', 'dinar'], ['Angola', 'kwanza'], ['Argentina', 'peso'], ['Armenia', 'dram'], ['Brazil', 'real'], ['Bulgaria', 'lev'], ['Cambodia', 'riel'], ['Canada', 'dollar'], ['Croatia', 'kuna'], ['Denmark', 'krone'], ['Europe', 'euro'], ['Hungary', 'forint'], ['India', 'rupee'], ['Iran', 'rial'], ['Japan', 'yen'], ['Korea', 'won'], ['Latvia', 'lats'], ['Lithuania', 'litas'], ['Macedonia', 'denar'], ['Malaysia', 'ringgit'], ['Mexico', 'peso'], ['Nigeria', 'naira'], ['Poland', 'zloty'], ['Romania', 'leu'], ['Russia', 'ruble'], ['Sweden', 'krona'], ['Thailand', 'baht'], ['Ukraine', 'hryvnia'], ['USA', 'dollar'], ['Vietnam', 'dong']], 'gram4-superlative': [['bad', 'worst'], ['big', 'biggest'], ['bright', 'brightest'], ['cold', 'coldest'], ['cool', 'coolest'], ['dark', 'darkest'], ['easy', 'easiest'], ['fast', 'fastest'], ['good', 'best'], ['great', 'greatest'], ['high', 'highest'], ['hot', 'hottest'], ['large', 'largest'], ['long', 'longest'], ['low', 'lowest'], ['lucky', 'luckiest'], ['old', 'oldest'], ['quick', 'quickest'], ['sharp', 'sharpest'], ['simple', 'simplest'], ['short', 'shortest'], ['slow', 'slowest'], ['small', 'smallest'], ['smart', 'smartest'], ['strange', 'strangest'], ['strong', 'strongest'], ['sweet', 'sweetest'], ['tall', 'tallest'], ['tasty', 'tastiest'], ['warm', 'warmest'], ['weak', 'weakest'], ['weird', 'weirdest'], ['wide', 'widest'], ['young', 'youngest']], 'gram6-nationality-adjective': [['Albania', 'Albanian'], ['Argentina', 'Argentinean'], ['Australia', 'Australian'], ['Austria', 'Austrian'], ['Belarus', 'Belorussian'], ['Brazil', 'Brazilian'], ['Bulgaria', 'Bulgarian'], ['Cambodia', 'Cambodian'], ['Chile', 'Chilean'], ['China', 'Chinese'], ['Colombia', 'Colombian'], ['Croatia', 'Croatian'], ['Denmark', 'Danish'], ['Egypt', 'Egyptian'], ['England', 'English'], ['France', 'French'], ['Germany', 'German'], ['Greece', 'Greek'], ['Iceland', 'Icelandic'], ['India', 'Indian'], ['Ireland', 'Irish'], ['Israel', 'Israeli'], ['Italy', 'Italian'], ['Japan', 'Japanese'], ['Korea', 'Korean'], ['Macedonia', 'Macedonian'], ['Malta', 'Maltese'], ['Mexico', 'Mexican'], ['Moldova', 'Moldovan'], ['Netherlands', 'Dutch'], ['Norway', 'Norwegian'], ['Peru', 'Peruvian'], ['Poland', 'Polish'], ['Portugal', 'Portuguese'], ['Russia', 'Russian'], ['Slovakia', 'Slovakian'], ['Spain', 'Spanish'], ['Sweden', 'Swedish'], ['Switzerland', 'Swiss'], ['Thailand', 'Thai'], ['Ukraine', 'Ukrainian']], 'gram7-past-tense': [['dancing', 'danced'], ['decreasing', 'decreased'], ['describing', 'described'], ['enhancing', 'enhanced'], ['falling', 'fell'], ['feeding', 'fed'], ['flying', 'flew'], ['generating', 'generated'], ['going', 'went'], ['hiding', 'hid'], ['hitting', 'hit'], ['implementing', 'implemented'], ['increasing', 'increased'], ['jumping', 'jumped'], ['knowing', 'knew'], ['listening', 'listened'], ['looking', 'looked'], ['moving', 'moved'], ['paying', 'paid'], ['playing', 'played'], ['predicting', 'predicted'], ['reading', 'read'], ['running', 'ran'], ['saying', 'said'], ['screaming', 'screamed'], ['seeing', 'saw'], ['selling', 'sold'], ['shrinking', 'shrank'], ['singing', 'sang'], ['sitting', 'sat'], ['sleeping', 'slept'], ['slowing', 'slowed'], ['spending', 'spent'], ['striking', 'struck'], ['swimming', 'swam'], ['taking', 'took'], ['thinking', 'thought'], ['vanishing', 'vanished'], ['walking', 'walked'], ['writing', 'wrote']], 'gram5-present-participle': [['code', 'coding'], ['dance', 'dancing'], ['debug', 'debugging'], ['decrease', 'decreasing'], ['describe', 'describing'], ['discover', 'discovering'], ['enhance', 'enhancing'], ['fly', 'flying'], ['generate', 'generating'], ['go', 'going'], ['implement', 'implementing'], ['increase', 'increasing'], ['invent', 'inventing'], ['jump', 'jumping'], ['listen', 'listening'], ['look', 'looking'], ['move', 'moving'], ['play', 'playing'], ['predict', 'predicting'], ['read', 'reading'], ['run', 'running'], ['say', 'saying'], ['scream', 'screaming'], ['see', 'seeing'], ['shuffle', 'shuffling'], ['sing', 'singing'], ['sit', 'sitting'], ['slow', 'slowing'], ['swim', 'swimming'], ['think', 'thinking'], ['vanish', 'vanishing'], ['walk', 'walking'], ['write', 'writing']], 'capital-world': [['Abuja', 'Nigeria'], ['Accra', 'Ghana'], ['Algiers', 'Algeria'], ['Amman', 'Jordan'], ['Ankara', 'Turkey'], ['Antananarivo', 'Madagascar'], ['Apia', 'Samoa'], ['Ashgabat', 'Turkmenistan'], ['Asmara', 'Eritrea'], ['Astana', 'Kazakhstan'], ['Athens', 'Greece'], ['Baghdad', 'Iraq'], ['Baku', 'Azerbaijan'], ['Bamako', 'Mali'], ['Bangkok', 'Thailand'], ['Banjul', 'Gambia'], ['Beijing', 'China'], ['Beirut', 'Lebanon'], ['Belgrade', 'Serbia'], ['Belmopan', 'Belize'], ['Berlin', 'Germany'], ['Bern', 'Switzerland'], ['Bishkek', 'Kyrgyzstan'], ['Bratislava', 'Slovakia'], ['Brussels', 'Belgium'], ['Bucharest', 'Romania'], ['Budapest', 'Hungary'], ['Bujumbura', 'Burundi'], ['Cairo', 'Egypt'], ['Canberra', 'Australia'], ['Caracas', 'Venezuela'], ['Chisinau', 'Moldova'], ['Conakry', 'Guinea'], ['Copenhagen', 'Denmark'], ['Dakar', 'Senegal'], ['Damascus', 'Syria'], ['Dhaka', 'Bangladesh'], ['Doha', 'Qatar'], ['Dublin', 'Ireland'], ['Dushanbe', 'Tajikistan'], ['Funafuti', 'Tuvalu'], ['Gaborone', 'Botswana'], ['Georgetown', 'Guyana'], ['Hanoi', 'Vietnam'], ['Harare', 'Zimbabwe'], ['Havana', 'Cuba'], ['Helsinki', 'Finland'], ['Islamabad', 'Pakistan'], ['Jakarta', 'Indonesia'], ['Kabul', 'Afghanistan'], ['Kampala', 'Uganda'], ['Kathmandu', 'Nepal'], ['Khartoum', 'Sudan'], ['Kiev', 'Ukraine'], ['Kigali', 'Rwanda'], ['Kingston', 'Jamaica'], ['Libreville', 'Gabon'], ['Lilongwe', 'Malawi'], ['Lima', 'Peru'], ['Lisbon', 'Portugal'], ['Ljubljana', 'Slovenia'], ['London', 'England'], ['Luanda', 'Angola'], ['Lusaka', 'Zambia'], ['Madrid', 'Spain'], ['Managua', 'Nicaragua'], ['Manama', 'Bahrain'], ['Manila', 'Philippines'], ['Maputo', 'Mozambique'], ['Minsk', 'Belarus'], ['Mogadishu', 'Somalia'], ['Monrovia', 'Liberia'], ['Montevideo', 'Uruguay'], ['Moscow', 'Russia'], ['Muscat', 'Oman'], ['Nairobi', 'Kenya'], ['Nassau', 'Bahamas'], ['Niamey', 'Niger'], ['Nicosia', 'Cyprus'], ['Nouakchott', 'Mauritania'], ['Nuuk', 'Greenland'], ['Oslo', 'Norway'], ['Ottawa', 'Canada'], ['Paramaribo', 'Suriname'], ['Paris', 'France'], ['Podgorica', 'Montenegro'], ['Quito', 'Ecuador'], ['Rabat', 'Morocco'], ['Riga', 'Latvia'], ['Rome', 'Italy'], ['Roseau', 'Dominica'], ['Santiago', 'Chile'], ['Skopje', 'Macedonia'], ['Sofia', 'Bulgaria'], ['Stockholm', 'Sweden'], ['Suva', 'Fiji'], ['Taipei', 'Taiwan'], ['Tallinn', 'Estonia'], ['Tashkent', 'Uzbekistan'], ['Tbilisi', 'Georgia'], ['Tegucigalpa', 'Honduras'], ['Tehran', 'Iran'], ['Thimphu', 'Bhutan'], ['Tirana', 'Albania'], ['Tokyo', 'Japan'], ['Tripoli', 'Libya'], ['Tunis', 'Tunisia'], ['Vaduz', 'Liechtenstein'], ['Valletta', 'Malta'], ['Vienna', 'Austria'], ['Vientiane', 'Laos'], ['Vilnius', 'Lithuania'], ['Warsaw', 'Poland'], ['Windhoek', 'Namibia'], ['Yerevan', 'Armenia'], ['Zagreb', 'Croatia']], 'gram1-adjective-to-adverb': [['amazing', 'amazingly'], ['apparent', 'apparently'], ['calm', 'calmly'], ['cheerful', 'cheerfully'], ['complete', 'completely'], ['efficient', 'efficiently'], ['fortunate', 'fortunately'], ['free', 'freely'], ['furious', 'furiously'], ['happy', 'happily'], ['immediate', 'immediately'], ['infrequent', 'infrequently'], ['lucky', 'luckily'], ['most', 'mostly'], ['obvious', 'obviously'], ['occasional', 'occasionally'], ['possible', 'possibly'], ['precise', 'precisely'], ['professional', 'professionally'], ['quick', 'quickly'], ['quiet', 'quietly'], ['rapid', 'rapidly'], ['rare', 'rarely'], ['reluctant', 'reluctantly'], ['safe', 'safely'], ['serious', 'seriously'], ['slow', 'slowly'], ['sudden', 'suddenly'], ['swift', 'swiftly'], ['typical', 'typically'], ['unfortunate', 'unfortunately'], ['usual', 'usually']]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1IHfWpuvoMW"
      },
      "source": [
        "\n",
        "# Tasks\n",
        "1. Write a function which when given one (capital city, country) training pair can predict the capital of the other countries in the capital-common-countries list in testtuples\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s51o3soAyGtP"
      },
      "source": [
        "2. Use the correct answers, also given, to evaluate how accurate your capital-predictor is. You should calculate the average accuracy over all possible training pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah8isEC7yGQh"
      },
      "source": [
        "def predict_captial(target_country, known_capital, known_country):\n",
        "  return mymodel.most_similar(positive=[target_country, known_capital], negative=[known_country])[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1QmRqZrtzwSi",
        "outputId": "f38706e7-5b38-4122-d2e9-aa9f3896ff9c"
      },
      "source": [
        "predict_captial('Spain', 'London', 'England')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Madrid'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aVC8ggz0kTI",
        "outputId": "f1d633d5-215f-4ce4-8632-b23319290cad"
      },
      "source": [
        "results = [predict_captial(country, 'London', 'England') == capital for [capital, country] in testtuples['capital-common-countries']]\n",
        "print(f'Accuracy: {results.count(True)/len(results):.2%}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 78.26%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxnHgrB4yIdv"
      },
      "source": [
        "3. Looking at your predictions, can you think of an easy way to improve performance?\n",
        "\n",
        "Repeat the predictions with other known vectors and select the mode prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI8wKp5s2ZBt"
      },
      "source": [
        "def predict_captial(target_country, known_pairs):\n",
        "  predictions = []\n",
        "  for (known_capital, known_country) in known_pairs:\n",
        "    predictions.append(mymodel.most_similar(positive=[target_country, known_capital], negative=[known_country])[0][0])\n",
        "  return max(set(predictions), key=predictions.count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPnmN43SyJZl",
        "outputId": "4481eea9-9e44-4ca1-ac2e-65ec2b0609c0"
      },
      "source": [
        "results = [predict_captial(country, [('London', 'England'), ('Paris', 'France'), ('Madrid', 'Spain')]) == capital for [capital, country] in testtuples['capital-common-countries']]\n",
        "print(f'Accuracy: {results.count(True)/len(results):.2%}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 82.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9UnkJAjyJpl"
      },
      "source": [
        "4. Adapt your code so that you can predict the country of which a city is capital. Is performance the same, higher or lower this way round?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqE82bgCyKjX"
      },
      "source": [
        "def predict_country(target_capital, known_capital, known_country):\n",
        "  return mymodel.most_similar(positive=[target_capital, known_country], negative=[known_capital])[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDbPf9G-9YxW",
        "outputId": "b3878923-47fd-46fa-a390-f1df858eba07"
      },
      "source": [
        "print(predict_country('Madrid', 'London', 'England'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1G3cbCD9aTz",
        "outputId": "2622e07c-9d01-43cd-a047-5caaeef6d3a4"
      },
      "source": [
        "results = [predict_country(capital, 'London', 'England') == country for [capital, country] in testtuples['capital-common-countries']]\n",
        "print(f'Accuracy: {results.count(True)/len(results):.2%}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 78.26%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7Wox1eT8kkj"
      },
      "source": [
        "def predict_country(target_capital, known_pairs):\n",
        "  predictions = []\n",
        "  for (known_capital, known_country) in known_pairs:\n",
        "    predictions.append(mymodel.most_similar(positive=[target_capital, known_country], negative=[known_capital])[0][0])\n",
        "  return max(set(predictions), key=predictions.count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXlMX3-U9bUy",
        "outputId": "d586640e-f4f1-4d53-9314-cc3fbe981781"
      },
      "source": [
        "results = [predict_country(capital, [('London', 'England'), ('Paris', 'France'), ('Madrid', 'Spain')]) == country for [capital, country] in testtuples['capital-common-countries']]\n",
        "print(f'Accuracy: {results.count(True)/len(results):.2%}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 82.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJmgGTZYyKqv"
      },
      "source": [
        "5. Adapt your code so that you can consider any of the relationships in testtuples. Rank the relationships in order of easiness to predict. Why do you think some are easier than others?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMod5wMX-XcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "779b75c3-332e-4a3c-a8bb-a8af104ad62c"
      },
      "source": [
        "import random\n",
        "n_train = 3\n",
        "n_test = 5\n",
        "\n",
        "ranks = {}\n",
        "\n",
        "for k in testtuples:\n",
        "  print(f'Processing {k}...')\n",
        "\n",
        "  relationships = testtuples[k]\n",
        "  sample = random.sample(relationships, n_train+n_test)\n",
        "  train = sample[:n_train]\n",
        "  test = sample[n_train:]\n",
        "\n",
        "  c = 0\n",
        "\n",
        "  for (target1, target2) in test:\n",
        "    predictions = []\n",
        "    for (known1, known2) in train:\n",
        "      predictions.append(mymodel.most_similar(positive=[target1,known2], negative=[known1])[0][0])\n",
        "    if max(set(predictions), key=predictions.count) == target2:\n",
        "      c += 1\n",
        "\n",
        "  ranks[k] = c/len(test)\n",
        "\n",
        "for k, v in sorted(ranks.items(), key=lambda x: x[1], reverse=True):\n",
        "  print(f'{k} has an accuracy of {v:.2%}')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing gram3-comparative...\n",
            "Processing gram8-plural...\n",
            "Processing capital-common-countries...\n",
            "Processing city-in-state...\n",
            "Processing family...\n",
            "Processing gram2-opposite...\n",
            "Processing currency...\n",
            "Processing gram4-superlative...\n",
            "Processing gram6-nationality-adjective...\n",
            "Processing gram7-past-tense...\n",
            "Processing gram5-present-participle...\n",
            "Processing capital-world...\n",
            "Processing gram1-adjective-to-adverb...\n",
            "gram3-comparative has an accuracy of 100.00%\n",
            "city-in-state has an accuracy of 100.00%\n",
            "family has an accuracy of 100.00%\n",
            "gram4-superlative has an accuracy of 100.00%\n",
            "gram6-nationality-adjective has an accuracy of 100.00%\n",
            "gram5-present-participle has an accuracy of 100.00%\n",
            "capital-world has an accuracy of 100.00%\n",
            "gram8-plural has an accuracy of 80.00%\n",
            "gram7-past-tense has an accuracy of 80.00%\n",
            "capital-common-countries has an accuracy of 60.00%\n",
            "gram1-adjective-to-adverb has an accuracy of 60.00%\n",
            "gram2-opposite has an accuracy of 40.00%\n",
            "currency has an accuracy of 20.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uo3fkR4HyWu"
      },
      "source": [
        "Accurate predictions depend on how frequently the 'known' word in the relationship is seen in the subcontext of the 'unknown' word. For example, comparing performance of capitals against currencies, capitals are likely talked about more frequently than their currencies. It also depends on the random sample used in my method, if economy in the selected countries is often mentioned in the text then it may produce better accuracy in its predictions. Another potential problem is other contexts of known 'positives', for example 'real' is the currency of Brazil but is used frequently in english with another meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0vRJXN6KA3v",
        "outputId": "5dca8894-f269-4af0-d40b-48c94a9eec07"
      },
      "source": [
        "pairs = [('Brazil', 'real'), ('Germany', 'euro'), ('Macedonia', 'denar')]\n",
        "for (known_country, known_currency) in pairs:\n",
        "  results = [mymodel.most_similar(positive=[country, known_currency], negative=[known_country])[0][0] == currency for [country, currency] in testtuples['currency']]\n",
        "  print(f'Predictions using {known_currency} from {known_country} had an accuracy of {results.count(True)/len(results):.2%}')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions using real from Brazil had an accuracy of 0.00%\n",
            "Predictions using euro from Germany had an accuracy of 46.67%\n",
            "Predictions using denar from Macedonia had an accuracy of 6.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvjqFcXDyL6h"
      },
      "source": [
        "6. A critic might say that the evaluation carried out in Mikolov et al. (2013) does not test the importance of the direction of the vector offset. London is close to England, so the vector difference is very small. Therefore, the method might do as well if it predicted the nearest neighbour of China as its capital. Implement this naive baseline which predicts the closest neighbour of the test item. Evaluate it for the different relationships in testtuples. Does it come close to doing as well as the vector offset method for any of the relationships?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PbNroNDEtKC",
        "outputId": "64c18e51-fbf3-487a-d3f6-184feb8ebcfb"
      },
      "source": [
        "n_train = 3\n",
        "n_test = 5\n",
        "\n",
        "ranks = {}\n",
        "\n",
        "for k in testtuples:\n",
        "  print(f'Processing {k}...')\n",
        "\n",
        "  relationships = testtuples[k]\n",
        "  sample = random.sample(relationships, n_train+n_test)\n",
        "  train = sample[:n_train]\n",
        "  test = sample[n_train:]\n",
        "\n",
        "  c = 0\n",
        "\n",
        "  for (target1, target2) in test:\n",
        "    if mymodel.most_similar(positive=[target1])[0][0] == target2:\n",
        "      c += 1\n",
        "\n",
        "  ranks[k] = c/len(test)\n",
        "\n",
        "for k, v in sorted(ranks.items(), key=lambda x: x[1], reverse=True):\n",
        "  print(f'{k} has an accuracy of {v:.2%}')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing gram3-comparative...\n",
            "Processing gram8-plural...\n",
            "Processing capital-common-countries...\n",
            "Processing city-in-state...\n",
            "Processing family...\n",
            "Processing gram2-opposite...\n",
            "Processing currency...\n",
            "Processing gram4-superlative...\n",
            "Processing gram6-nationality-adjective...\n",
            "Processing gram7-past-tense...\n",
            "Processing gram5-present-participle...\n",
            "Processing capital-world...\n",
            "Processing gram1-adjective-to-adverb...\n",
            "gram8-plural has an accuracy of 100.00%\n",
            "gram3-comparative has an accuracy of 60.00%\n",
            "gram6-nationality-adjective has an accuracy of 60.00%\n",
            "gram5-present-participle has an accuracy of 40.00%\n",
            "city-in-state has an accuracy of 20.00%\n",
            "family has an accuracy of 20.00%\n",
            "gram7-past-tense has an accuracy of 20.00%\n",
            "capital-world has an accuracy of 20.00%\n",
            "capital-common-countries has an accuracy of 0.00%\n",
            "gram2-opposite has an accuracy of 0.00%\n",
            "currency has an accuracy of 0.00%\n",
            "gram4-superlative has an accuracy of 0.00%\n",
            "gram1-adjective-to-adverb has an accuracy of 0.00%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}