{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2V2.ipynb",
      "provenance": [],
      "mount_file_id": "1LQxXICF0mV19o2n78TrkTgykmTaZuxJF",
      "authorship_tag": "ABX9TyMYgWL2auHbsgh6X6/OZbMv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashmcmn/ANLE-Labs/blob/main/week2/Lab2V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZr28FX2LIcG"
      },
      "source": [
        "# 1 Getting started \n",
        "During labs this term, you are going to be exploring language models and applying them to the Mi crosoft Research Sentence Completion Challenge (Zweig and Burges, 2011). Today, you are going to start by building an n-gram language model based on the training corpus for the Challenge. All of the data for the Sentence Completion Challenge is contained in this Lab2resources directory in the sub-directory titled sentence-completion. You may need to unzip training data.tgz so that the Holmes Training Data sub-directory is visible. \n",
        "You can use the following code to list the directory and select 50% of the files (holding back the other 50% so that you can evaluate your language model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDH-cYff0cV4"
      },
      "source": [
        "import nltk\n",
        "!gdown --id '1iif5UfVD-z5wCmXuKzp7zGyz22L_mlii'\n",
        "!unzip 'lab2data.zip'\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mzM9czUKiQY"
      },
      "source": [
        "import os,random,math \n",
        "from nltk import word_tokenize as tokenize \n",
        "import operator\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "TRAINING_DIR='lab2data/Holmes_Training_Data'\n",
        "\n",
        "def get_training_testing(training_dir=TRAINING_DIR,split=0.5,amount=6): \n",
        "  filenames=os.listdir(training_dir) \n",
        "  random.shuffle(filenames)\n",
        "  selection=random.sample(filenames, amount) \n",
        "  index=int(amount*split) \n",
        "  return(selection[:index],selection[index:]) \n",
        "\n",
        "trainingfiles,heldoutfiles=get_training_testing()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gih5ZqMMMNc3"
      },
      "source": [
        "# 2 A Unigram Model \n",
        "The code below implements a simple unigram model. The class stores the unigram probabliity distribution and provide methods for training and lookup. \n",
        "\n",
        "Look through the code and you should see the following steps in the training process. \n",
        "*   Each of the subset of datafiles is processed in turn. \n",
        "*   In each datafile, each line is tokenized using nltk.word tokenize() \n",
        "*   START and END tokens are added to the beginning and end of each line. \n",
        "*   A count is kept of how many times each token occurs. This is stored in a dictionary self.unigram \n",
        "*   Once all of the datafiles have been processed, \n",
        "the counts are converted into probabilities (by dividing all of them by the sum of all of the values in the dictionary). \n",
        "\n",
        "Test your model by looking up the probabilities of some (known) words. At this stage, if the word was not seen in the training data, the probability returned should be zero (we will worry about smoothing later!) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAhgI-wZMNw4"
      },
      "source": [
        "class language_model(): \n",
        "  def __init__(self, trainingdir=TRAINING_DIR, files=[], verbose=True): \n",
        "    self.training_dir = trainingdir \n",
        "    self.files = files \n",
        "    self.verbose = verbose\n",
        "    self.train() \n",
        "\n",
        "  def train(self): \n",
        "    self.unigram={} \n",
        "    self._processfiles() \n",
        "    self._convert_to_probs() \n",
        "\n",
        "  def _processline(self, line): \n",
        "    tokens = ['_START'] + tokenize(line) + ['_END'] \n",
        "    for token in tokens: \n",
        "      self.unigram[token] = self.unigram.get(token, 0) + 1 \n",
        "\n",
        "  def _processfiles(self): \n",
        "    for afile in self.files:\n",
        "      if self.verbose: \n",
        "        print('Processing {}'.format(afile)) \n",
        "      try: \n",
        "        with open(os.path.join(self.training_dir, afile)) as instream: \n",
        "          for line in instream: \n",
        "            line = line.rstrip() \n",
        "            if len(line) > 0: \n",
        "              self._processline(line) \n",
        "      except UnicodeDecodeError: \n",
        "        print('UnicodeDecodeError processing {}: ignoring file'.format(afile)) \n",
        "\n",
        "  def _convert_to_probs(self): \n",
        "    self.unigram={w:f / sum(self.unigram.values()) for (w, f) in self.unigram.items()} \n",
        "\n",
        "  def get_prob(self, token, method='unigram'): \n",
        "    if method == 'unigram': \n",
        "      return self.unigram.get(token,0) \n",
        "    else: \n",
        "      print('Not implemented: {}'.format(method)) \n",
        "      return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC4NGcUzNMZV"
      },
      "source": [
        "mylm=language_model(files=trainingfiles) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnxPpQV-1qNW"
      },
      "source": [
        "mylm.get_prob('man')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQRF3OArO6Az"
      },
      "source": [
        "# 2.2 Generation \n",
        "Add some functionality to your class so that you can generate a string of highly probable words. You can do this by sorting the dictionary of unigram probabilities and then randomly choosing 1 of the top k words. Repeatedly choose words until a particular token is generated (e.g., ’.’) or a maximum length is exceeded. \n",
        "\n",
        "As an extension, you could try to sample from the distribution. Assign each of the words a range of numbers - the size of which is proportional to its probability. You can do this by considering the cumulative probability distribution (iterate through the words in the distribution, adding the probability of the current word to the sum of all probabilities seen so far). Then you just need to pick a random number and select the word which has this number in its assigned range. Of course, there are library methods which can do this for you to — check out random.choices() at https://www.kite.com/python/ answers/how-to-sample-a-random-number-from-a-probability-distribution-in-python "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPaD56IW10xf"
      },
      "source": [
        "def generate_sentence(self, n=None, method='unigram'):\n",
        "  k = 10\n",
        "  blacklist = ['_START', '_END']\n",
        "  if method == 'unigram':\n",
        "    words = {k:v for k,v in self.unigram.items() if k not in blacklist}\n",
        "    top_k = sorted(words, key=words.get, reverse=True)[:k]\n",
        "    sentence = [random.choice(top_k)]\n",
        "\n",
        "    if n == None:\n",
        "      print('Parameter \\'n\\' must be provided when generating sentence from the unigram model.')\n",
        "      return\n",
        "\n",
        "    while len(sentence) < n:\n",
        "      words = {k:v for k,v in self.unigram.items() if k not in blacklist}\n",
        "      top_k = sorted(words, key=words.get, reverse=True)[:k]\n",
        "      sentence.append(random.choice(top_k))\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "  else:\n",
        "    print('Not implemented: {}'.format(method)) \n",
        "    return\n",
        "\n",
        "language_model.generate_sentence = generate_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzFhZyNL23wF"
      },
      "source": [
        "mylm.generate_sentence(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn613MRaPLYX"
      },
      "source": [
        "# 3 Adding Bigrams \n",
        "# 3.1 Training \n",
        "Extend your class so that, at training time, you also store bigram counts. I suggest that you use a dictionary of dictionaries for the bigram counts. The key for the outer dictionary should be the previous\n",
        "word (which may be a START token) and the key for the inner dictionary should be the current word. The value is then how many times the current word has occurred after the previous word. For example, if you see the sentence ”When did the cat sit on the mat?”, you should get the following dictionary: \n",
        "\n",
        "bigram={_START:{When:1}, \n",
        "When:{did:1}, \n",
        "did:{the:1}, \n",
        "the:{cat:1, mat: 1}, \n",
        "cat:{sit:1}, \n",
        "sit:{on:1}, \n",
        "on:{the:1}, \n",
        "mat:{?:1},} \n",
        "\n",
        "You need to normalise each of the internal dictionaries to create probability distributions for P(w2|w1). You do this by summing the values in each internal dictionary and then dividing each count by the sum of values for that dictionary. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnN9iZLo6KLQ"
      },
      "source": [
        "def train(self): \n",
        "  self.unigram={} \n",
        "  self.bigram={}\n",
        "\n",
        "  self._processfiles() \n",
        "  self._convert_to_probs() \n",
        "\n",
        "def _processline(self, line): \n",
        "  tokens = ['_START'] + tokenize(line) + ['_END']\n",
        "  prevToken = '_END'\n",
        "\n",
        "  for i, token in enumerate(tokens):\n",
        "    self.unigram[token] = self.unigram.get(token, 0) + 1\n",
        "\n",
        "    tokenDict = self.bigram.get(prevToken, {})\n",
        "    tokenDict[token] = tokenDict.get(token, 0) + 1\n",
        "    self.bigram[prevToken] = tokenDict\n",
        "    prevToken = token\n",
        "\n",
        "def _convert_to_probs(self): \n",
        "  self.unigram = {w:f / sum(self.unigram.values()) for (w, f) in self.unigram.items()} \n",
        "  self.bigram = {w1:{w2:f / sum(words.values()) for (w2, f) in words.items()} for (w1, words) in self.bigram.items()}\n",
        "\n",
        "def get_prob(self, token, token2=None, method='unigram'): \n",
        "  if method == 'unigram': \n",
        "    return self.unigram.get(token,0) \n",
        "  elif method == 'bigram':\n",
        "    return self.bigram.get(token2,{}).get(token,0)\n",
        "  else: \n",
        "    print('Not implemented: {}'.format(method)) \n",
        "    return\n",
        "\n",
        "language_model.train = train\n",
        "language_model._processline = _processline\n",
        "language_model._convert_to_probs = _convert_to_probs\n",
        "language_model.get_prob = get_prob\n",
        "mylm.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8YuL1OgYlee"
      },
      "source": [
        "mylm.get_prob('man', 'The', 'bigram')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sURR8rTUb6pu"
      },
      "source": [
        "# 3.2 Generation \n",
        "Can you use your bigram probabilities to generate plausible sounding sentences? Initialise the current token as START and then randomly choose one of the top k most probable words for the next token. Then use this as the current token to generate the next token and so on. Experiment with different values of k "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HyoB2iLLM9W"
      },
      "source": [
        "def generate_sentence(self, n=None, method='unigram'):\n",
        "  k = 10\n",
        "  blacklist = ['_START']\n",
        "  if method == 'unigram':\n",
        "    words = {k:v for k,v in self.unigram.items() if k not in blacklist+['_END']}\n",
        "    top_k = sorted(words, key=words.get, reverse=True)[:k]\n",
        "    sentence = [random.choice(top_k)]\n",
        "\n",
        "    if n == None:\n",
        "      print('Parameter \\'n\\' must be provided when generating sentence from the unigram model.')\n",
        "      return\n",
        "\n",
        "    while len(sentence) < n:\n",
        "      words = {k:v for k,v in self.unigram.items() if k not in blacklist+['_END']}\n",
        "      top_k = sorted(words, key=words.get, reverse=True)[:k]\n",
        "      sentence.append(random.choice(top_k))\n",
        "      \n",
        "    return ' '.join(sentence)\n",
        "  elif method == 'bigram':\n",
        "    words = {k:v for k,v in self.bigram['_START'].items() if k not in blacklist}\n",
        "    top_k = sorted(words, key=words.get, reverse=True)[:k]\n",
        "    sentence = [random.choice(top_k)]\n",
        "\n",
        "    if n == None:\n",
        "      while sentence[-1] != '_END':\n",
        "        words = {k:v for k,v in self.bigram[sentence[-1]].items() if k not in blacklist}\n",
        "        top_k = sorted(words, key=words.get, reverse=True)[:k]\n",
        "        sentence.append(random.choice(top_k))\n",
        "\n",
        "      sentence.pop()\n",
        "    else:\n",
        "      while len(sentence) < n:\n",
        "        words = {k:v for k,v in self.bigram[sentence[-1]].items() if k not in blacklist+['_END']}\n",
        "        top_k = sorted(words, key=words.get, reverse=True)[:k]\n",
        "        sentence.append(random.choice(top_k))\n",
        "    return ' '.join(sentence)\n",
        "  else:\n",
        "    print('Not implemented: {}'.format(method)) \n",
        "    return\n",
        "\n",
        "language_model.generate_sentence = generate_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPgmFt7hvkwW"
      },
      "source": [
        "mylm.generate_sentence(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBMmaAppvnQU"
      },
      "source": [
        "mylm.generate_sentence(method='bigram')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIuAgImSvoCr"
      },
      "source": [
        "mylm.generate_sentence(10, method='bigram')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa15yhhUPUAq"
      },
      "source": [
        "# 4 Perplexity \n",
        "Perplexity is a function of a model and some language data. If the observed language data is highly likely given the model then the perplexity will be low. \n",
        "* Write a method or function that calculates the log probability (LP) of a corpus using the unigram model and the bigram model. \n",
        "* Compute the perplexity using the formula p = e^(-lp/N) , where N is the number of words in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8D49bRpWteC"
      },
      "source": [
        "def _lp_line(self, line, method='unigram'):\n",
        "  tokens = ['_START']+tokenize(line)+['_END']\n",
        "  lp = 0\n",
        "\n",
        "  for i, token in enumerate(tokens[1:]):\n",
        "    lp += np.log(self.get_prob(token, tokens[i], method))\n",
        "  \n",
        "  return lp, len(tokens[1:])\n",
        "\n",
        "def calculate_perplexity(self, corpus, method='unigram'):\n",
        "  p, N = 0, 0\n",
        "\n",
        "  for line in corpus:\n",
        "    line = line.rstrip()\n",
        "    if len(line) > 0:\n",
        "      res = self._lp_line(line, method)\n",
        "      p += res[0]\n",
        "      N += res[1]\n",
        "\n",
        "  return np.exp(-p/N)\n",
        "\n",
        "language_model._lp_line = _lp_line\n",
        "language_model.calculate_perplexity = calculate_perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwf0_HpvrqKN"
      },
      "source": [
        "for afile in trainingfiles:\n",
        "  print(\"Processing file {}\".format(afile))\n",
        "  try:\n",
        "    with open(os.path.join(TRAINING_DIR, afile)) as instream:\n",
        "      print(mylm.calculate_perplexity(instream))\n",
        "  except UnicodeDecodeError:\n",
        "    print('UnicodeDecodeError processing file {}: ignoring rest of file'.format(afile))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjPLOgFsPoFk"
      },
      "source": [
        "# 5 Dealing with Unseen Data \n",
        "If you run your perplexity function on the testingfiles (rather than the training files), you will quickly run into the problem that certain unigrams and bigrams in the testingfiles were not present in the trainingfiles. This means that their probability will be zero (making the probability of the whole corpus to be zero). \n",
        "\n",
        "# 5.1 Unknown words \n",
        "In order to have some probability mass for unseen unigrams, we need to pretend that some were unseen in the training data. Therefore, we replace low frequency unigrams in the training data with the UNK token. One way of implementing this is: \n",
        "* Before converting counts to probabilities, iterate over the unigram probability distribution and find words which have occurred less frequently than a threshold (e.g., 2). Remove this key from the unigram distribution del self.unigram[key] and add the count to the entry for the UNK token. \n",
        "* Iterate over the bigram distribution. \n",
        "– For each inner dictionary, use the same process as for the unigram distribution. – If any outer keys are no longer valid unigram keys, delete that outer key and add the whole contents of the dictionary associated with it to an entry in the bigram distribution for UNK \n",
        "* When looking up a probability or a distribution for a word, if it is not found then use the probability or distribution for UNK instead \n",
        "You should now be able to compute the perplexity of the unigram model for the testingfiles. What do you notice about the perplexity values for this model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo357gPTfKr0"
      },
      "source": [
        "def _add_unknowns(self, k=2):\n",
        "  for word, freq in list(self.unigram.items()):\n",
        "    if freq < k:\n",
        "      del self.unigram[word]\n",
        "      self.unigram['_UNK'] = self.unigram.get('_UNK', 0) + freq\n",
        "  \n",
        "  for word, inner in list(self.bigram.items()):\n",
        "    for word2, freq in list(inner.items()):\n",
        "      if self.unigram.get(word2, 0) == 0:\n",
        "        inner['_UNK'] = inner.get('_UNK', 0) + freq\n",
        "        del inner[word2]\n",
        "    \n",
        "    if self.unigram.get(word, 0) == 0:\n",
        "      del self.bigram[word]\n",
        "      b = self.bigram.get('_UNK', {})\n",
        "      b.update(inner)\n",
        "      self.bigram['_UNK'] = b\n",
        "    else:\n",
        "      self.bigram[word] = inner\n",
        "\n",
        "def train(self): \n",
        "  self.unigram={} \n",
        "  self.bigram={}\n",
        "  \n",
        "  self._processfiles()\n",
        "  self._add_unknowns()\n",
        "  self._convert_to_probs() \n",
        "\n",
        "def get_prob(self, token, token2=None, method='unigram'): \n",
        "  if method == 'unigram': \n",
        "    return self.unigram.get(token, self.unigram.get('_UNK', 0))\n",
        "  elif method == 'bigram':\n",
        "    b = self.bigram.get(token2, self.bigram.get('_UNK', {}))\n",
        "    return b.get(token, b.get('_UNK', 0))\n",
        "  else: \n",
        "    print('Not implemented: {}'.format(method)) \n",
        "    return\n",
        "\n",
        "\n",
        "language_model._add_unknowns = _add_unknowns\n",
        "language_model.train = train\n",
        "language_model.get_prob = get_prob\n",
        "mylm.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qJOsYMa7dQe"
      },
      "source": [
        "for afile in trainingfiles:\n",
        "  print(\"Processing file {}\".format(afile))\n",
        "  try:\n",
        "    with open(os.path.join(TRAINING_DIR, afile)) as instream:\n",
        "      print(mylm.calculate_perplexity(instream))\n",
        "  except UnicodeDecodeError:\n",
        "    print('UnicodeDecodeError processing file {}: ignoring rest of file'.format(afile))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr1Q-YeuMngF"
      },
      "source": [
        "# 5.2 Discounting for unseen combinations \n",
        "Having dealt with unseen unigrams, we need to also deal with unseen bigrams i.e., bigrams where both words have been seen but not with each other. \n",
        "\n",
        "We could use Laplacian (add-k) smoothing but this is both computationally inefficient and it tends to assign too much probability mass to unseen events. A more common approach is to use some form of discounting and backoff. \n",
        "\n",
        "First, we apply a discount to each bigram count. There are variations where the discount applied depends on the word or the frequency of the word or the frequency of the bigram, but applying a fixed or absolute discount of 0.75 to each bigram count is the simplest method and works very well. It is also necessary to store the total amount that has been discounted for each unigram (this will depend on how many words have been seen following it). You can store this in the bigram distribution (under self.bigram[unigram][\" DISCOUNT\"]. This means the total counts will not have changed and therefore you have effectively reserved some probability mass for unseen bigrams. \n",
        "\n",
        "Then, to estimate the smoothed probability of Pe(w2|w1): \n",
        "Pe(w2|w1) = Po(w2|w1) + Po( DISCOUNT |w1) × Po(w2) \n",
        "You should now be able to compute the perplexity of the bigram model with respect to the testing files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyhR65HJWwN3"
      },
      "source": [
        "def train(self): \n",
        "  self.unigram={}\n",
        "  self.bigram={}\n",
        "\n",
        "  self._processfiles()\n",
        "  self._add_unknowns()\n",
        "  self._discount()\n",
        "  self._convert_to_probs() \n",
        "\n",
        "def _discount(self, d=0.75):\n",
        "  for word1 in self.bigram:\n",
        "    for word2 in self.bigram[word1]:\n",
        "      self.bigram[word1][word2] -= d\n",
        "    self.bigram[word1]['_DISCOUNT'] = d * len(self.bigram[word1])\n",
        "\n",
        "def get_prob(self, token, token2=None, method='unigram'): \n",
        "  if method == 'unigram': \n",
        "    return self.unigram.get(token, self.unigram.get('_UNK', 0))\n",
        "  elif method == 'bigram':\n",
        "    b = self.bigram.get(token2, self.bigram.get('_UNK', {}))\n",
        "    p1 = b.get(token, b.get('_UNK', 0))\n",
        "    p2 = b['_DISCOUNT']\n",
        "    p3 = self.unigram.get(token, self.unigram.get('_UNK', 0))\n",
        "    if p1 < 0:\n",
        "      print(token, token2)\n",
        "    return p1 + p2 * p3\n",
        "  else: \n",
        "    print('Not implemented: {}'.format(method)) \n",
        "    return\n",
        "\n",
        "language_model._discount = _discount\n",
        "language_model.train = train\n",
        "language_model.get_prob = get_prob\n",
        "mylm.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtGv3qNJWx8E"
      },
      "source": [
        "for afile in trainingfiles:\n",
        "  print(\"Processing file {}\".format(afile))\n",
        "  try:\n",
        "    with open(os.path.join(TRAINING_DIR, afile)) as instream:\n",
        "      print(mylm.calculate_perplexity(instream, method='bigram'))\n",
        "  except UnicodeDecodeError:\n",
        "    print('UnicodeDecodeError processing file {}: ignoring rest of file'.format(afile))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ-imtGEQRVJ"
      },
      "source": [
        "# 6 Extensions \n",
        "1. Implement the Kneser-Ney backoff method and compare this to the absolute discounting method introduced above. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fAlogMMBRcP"
      },
      "source": [
        "def _discount(self, d=0.75):\n",
        "  self.kn = {}\n",
        "\n",
        "  for word1 in self.bigram:\n",
        "    for word2 in self.bigram[word1]:\n",
        "      self.bigram[word1][word2] -= d\n",
        "      self.kn[word2] = self.kn.get(word2, 0) + 1\n",
        "    self.bigram[word1]['_DISCOUNT'] = d * len(self.bigram[word1])\n",
        "\n",
        "def _convert_to_probs(self): \n",
        "  self.unigram = {w:f / sum(self.unigram.values()) for (w, f) in self.unigram.items()} \n",
        "  self.bigram = {w1:{w2:f / sum(words.values()) for (w2, f) in words.items()} for (w1, words) in self.bigram.items()}\n",
        "  self.kn = {w:f / sum(self.kn.values()) for (w,f) in self.kn.items()}\n",
        "\n",
        "def get_prob(self, token, token2=None, method='unigram', smoothing=True): \n",
        "  if method == 'unigram': \n",
        "    return self.unigram.get(token, self.unigram.get('_UNK', 0))\n",
        "  elif method == 'bigram':\n",
        "    d = self.kn if smoothing else self.unigram\n",
        "    b = self.bigram.get(token2, self.bigram.get('_UNK', {}))\n",
        "    p1 = b.get(token, b.get('_UNK', 0))\n",
        "    p2 = b['_DISCOUNT']\n",
        "    p3 = d.get(token, d.get('_UNK', 0))\n",
        "    return p1 + p2 * p3\n",
        "  else: \n",
        "    print('Not implemented: {}'.format(method)) \n",
        "    return\n",
        "\n",
        "def _lp_line(self, line, method='unigram', smoothing=True):\n",
        "  tokens = ['_START']+tokenize(line)+['_END']\n",
        "  lp = 0\n",
        "\n",
        "  for i, token in enumerate(tokens[1:]):\n",
        "    lp += np.log(self.get_prob(token, tokens[i], method, smoothing))\n",
        "  \n",
        "  return lp, len(tokens[1:])\n",
        "\n",
        "def calculate_perplexity(self, corpus, method='unigram', smoothing=True):\n",
        "  p, N = 0, 0\n",
        "\n",
        "  for line in corpus:\n",
        "    line = line.rstrip()\n",
        "    if len(line) > 0:\n",
        "      res = self._lp_line(line, method, smoothing)\n",
        "      p += res[0]\n",
        "      N += res[1]\n",
        "\n",
        "  return np.exp(-p/N)\n",
        "\n",
        "language_model._discount = _discount\n",
        "language_model._convert_to_probs = _convert_to_probs\n",
        "language_model.get_prob = get_prob\n",
        "language_model._lp_line = _lp_line\n",
        "language_model.calculate_perplexity = calculate_perplexity\n",
        "mylm.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wfO84R9Dps7"
      },
      "source": [
        "for afile in trainingfiles:\n",
        "  print(\"Processing file {}\".format(afile))\n",
        "  try:\n",
        "    with open(os.path.join(TRAINING_DIR, afile)) as instream:\n",
        "      print(mylm.calculate_perplexity(instream, method='bigram'))\n",
        "  except UnicodeDecodeError:\n",
        "    print('UnicodeDecodeError processing file {}: ignoring rest of file'.format(afile))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyIljdXEBOJ3"
      },
      "source": [
        "2. Investigate how perplexity is affected by one or more of the following \n",
        "  * amount of training data \n",
        "  * amount of testing data \n",
        "  * the threshold frequency for unknown words \n",
        "  * the size of the discount applied to observed bigrams \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD6xPXoOXhX3"
      },
      "source": [
        "def __init__(self, trainingdir=TRAINING_DIR, files=[], verbose=True, k=2, d=0.75): \n",
        "  self.training_dir = trainingdir \n",
        "  self.files = files \n",
        "  self.verbose = verbose\n",
        "  self.train(k, d) \n",
        "\n",
        "def train(self, k, d): \n",
        "  self.unigram={}\n",
        "  self.bigram={}\n",
        "\n",
        "  self._processfiles()\n",
        "  self._add_unknowns(k)\n",
        "  self._discount(d)\n",
        "  self._convert_to_probs() \n",
        "\n",
        "def _add_unknowns(self, k):\n",
        "  for word, freq in list(self.unigram.items()):\n",
        "    if freq < k:\n",
        "      del self.unigram[word]\n",
        "      self.unigram['_UNK'] = self.unigram.get('_UNK', 0) + freq\n",
        "  \n",
        "  for word, inner in list(self.bigram.items()):\n",
        "    for word2, freq in list(inner.items()):\n",
        "      if self.unigram.get(word2, 0) == 0:\n",
        "        inner['_UNK'] = inner.get('_UNK', 0) + freq\n",
        "        del inner[word2]\n",
        "    \n",
        "    if self.unigram.get(word, 0) == 0:\n",
        "      del self.bigram[word]\n",
        "      b = self.bigram.get('_UNK', {})\n",
        "      b.update(inner)\n",
        "      self.bigram['_UNK'] = b\n",
        "    else:\n",
        "      self.bigram[word] = inner\n",
        "\n",
        "def _discount(self, d):\n",
        "  self.kn = {}\n",
        "\n",
        "  for word1 in self.bigram:\n",
        "    for word2 in self.bigram[word1]:\n",
        "      self.bigram[word1][word2] -= d\n",
        "      self.kn[word2] = self.kn.get(word2, 0) + 1\n",
        "    self.bigram[word1]['_DISCOUNT'] = d * len(self.bigram[word1])\n",
        "\n",
        "language_model.__init__ = __init__\n",
        "language_model.train = train\n",
        "language_model._add_unknowns = _add_unknowns\n",
        "language_model._discount = _discount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwzdmcUULHsp"
      },
      "source": [
        "splits = [0.2, 0.6, 0.9]\n",
        "thresholds = [2, 4, 5]\n",
        "discounts = [0.3, 0.6, 0.9]\n",
        "\n",
        "results = []\n",
        "\n",
        "for split in splits:\n",
        "  for k in thresholds:\n",
        "    for d in discounts:\n",
        "      trainingfiles,heldoutfiles=get_training_testing(split=split,amount=10)\n",
        "      mylm = language_model(files=trainingfiles, verbose=False, k=k, d=d)\n",
        "      p = 0\n",
        "      n = len(heldoutfiles)\n",
        "      for afile in heldoutfiles:\n",
        "        try:\n",
        "          with open(os.path.join(TRAINING_DIR, afile)) as instream:\n",
        "            p += mylm.calculate_perplexity(instream, method='bigram')\n",
        "        except UnicodeDecodeError:\n",
        "          print('UnicodeDecodeError processing file {}: ignoring rest of file'.format(afile))\n",
        "          n -= 1\n",
        "      if n == 0:\n",
        "        print('Skipping test due to lack of test data')\n",
        "        continue\n",
        "      p /= n\n",
        "      results.append([split, k, d, p])\n",
        "      print('Split: {}, Threshold: {}, Discount: {}, Average Perplexity: {}'.format(split, k, d, p))\n",
        "\n",
        "df = pd.DataFrame(results, columns=['Split', 'Threshold', 'Discount', 'Perplexity'])\n",
        "df.sort_values(by=['Perplexity'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D9vJhKDBOUO"
      },
      "source": [
        "3. Extend your model to trigrams. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExmE70QshLL1"
      },
      "source": [
        "class language_model(): \n",
        "  def __init__(self, trainingdir=TRAINING_DIR, files=[], verbose=True, k=2, d=0.75): \n",
        "    self.training_dir = trainingdir \n",
        "    self.files = files \n",
        "    self.verbose = verbose\n",
        "    self.train(k, d) \n",
        "\n",
        "  def train(self, k, d): \n",
        "    self.unigram={}\n",
        "    self.bigram={}\n",
        "    self.trigram={}\n",
        "\n",
        "    self._processfiles()\n",
        "    self._add_unknowns(k)\n",
        "    self._discount(d)\n",
        "    self._convert_to_probs() \n",
        "\n",
        "  def _processline(self, line): \n",
        "    tokens = ['_START'] + tokenize(line) + ['_END']\n",
        "    prevToken = '_END'\n",
        "    \n",
        "    for i, token in enumerate(tokens):\n",
        "      self.unigram[token] = self.unigram.get(token, 0) + 1\n",
        "\n",
        "      tokenDict = self.bigram.get(prevToken, {})\n",
        "      tokenDict[token] = tokenDict.get(token, 0) + 1\n",
        "      self.bigram[prevToken] = tokenDict\n",
        "\n",
        "      if i < len(tokens) - 1:\n",
        "        nextToken = tokens[i+1]\n",
        "        tokenDict = self.trigram.get(prevToken, {})\n",
        "        tokenDict[token] = tokenDict.get(token, 0) + 1\n",
        "        tokenDict[nextToken] = tokenDict.get(nextToken, 0) + 1\n",
        "        self.trigram[prevToken] = tokenDict\n",
        "\n",
        "      prevToken = token\n",
        "\n",
        "  def _processfiles(self): \n",
        "    for afile in self.files:\n",
        "      if self.verbose: \n",
        "        print('Processing {}'.format(afile)) \n",
        "      try: \n",
        "        with open(os.path.join(self.training_dir, afile)) as instream: \n",
        "          for line in instream: \n",
        "            line = line.rstrip() \n",
        "            if len(line) > 0: \n",
        "              self._processline(line) \n",
        "      except UnicodeDecodeError: \n",
        "        print('UnicodeDecodeError processing {}: ignoring file'.format(afile)) \n",
        "\n",
        "  def _convert_to_probs(self): \n",
        "    self.unigram = {w:f / sum(self.unigram.values()) for (w, f) in self.unigram.items()} \n",
        "    self.bigram = {w1:{w2:f / sum(words.values()) for (w2, f) in words.items()} for (w1, words) in self.bigram.items()}\n",
        "    self.trigram = {w1:{w2:f / sum(words.values()) for (w2, f) in words.items()} for (w1, words) in self.trigram.items()}\n",
        "    self.kn = {w:f / sum(self.kn.values()) for (w,f) in self.kn.items()}\n",
        "\n",
        "  def get_prob(self, token, token2=None, method='unigram', smoothing=True): \n",
        "    if method == 'unigram': \n",
        "      return self.unigram.get(token, self.unigram.get('_UNK', 0))\n",
        "    elif method == 'bigram':\n",
        "      d = self.kn if smoothing else self.unigram\n",
        "      b = self.bigram.get(token2, self.bigram.get('_UNK', {}))\n",
        "      p1 = b.get(token, b.get('_UNK', 0))\n",
        "      p2 = b['_DISCOUNT']\n",
        "      p3 = d.get(token, d.get('_UNK', 0))\n",
        "      return p1 + p2 * p3\n",
        "    elif method == 'trigram':\n",
        "      b = self.trigram.get(token2, self.trigram.get('_UNK', {}))\n",
        "      p1 = b.get(token, b.get('_UNK', 0))\n",
        "      p2 = b['_DISCOUNT']\n",
        "      p3 = b.get(token, b.get('_UNK', 0))\n",
        "      return p1 + p2 * p3\n",
        "    else: \n",
        "      print('Not implemented: {}'.format(method)) \n",
        "      return\n",
        "\n",
        "  def _lp_line(self, line, method='unigram', smoothing=True):\n",
        "    tokens = ['_START']+tokenize(line)+['_END']\n",
        "    lp = 0\n",
        "\n",
        "    for i, token in enumerate(tokens[1:]):\n",
        "      lp += np.log(self.get_prob(token, tokens[i], method, smoothing))\n",
        "    \n",
        "    return lp, len(tokens[1:])\n",
        "\n",
        "  def calculate_perplexity(self, corpus, method='unigram', smoothing=True):\n",
        "    p, N = 0, 0\n",
        "\n",
        "    for line in corpus:\n",
        "      line = line.rstrip()\n",
        "      if len(line) > 0:\n",
        "        res = self._lp_line(line, method, smoothing)\n",
        "        p += res[0]\n",
        "        N += res[1]\n",
        "\n",
        "    return np.exp(-p/N)\n",
        "\n",
        "  def _add_unknowns(self, k):\n",
        "    for word, freq in list(self.unigram.items()):\n",
        "      if freq < k:\n",
        "        del self.unigram[word]\n",
        "        self.unigram['_UNK'] = self.unigram.get('_UNK', 0) + freq\n",
        "    \n",
        "    for word, inner in list(self.bigram.items()):\n",
        "      for word2, freq in list(inner.items()):\n",
        "        if self.unigram.get(word2, 0) == 0:\n",
        "          inner['_UNK'] = inner.get('_UNK', 0) + freq\n",
        "          del inner[word2]\n",
        "      \n",
        "      if self.unigram.get(word, 0) == 0:\n",
        "        del self.bigram[word]\n",
        "        b = self.bigram.get('_UNK', {})\n",
        "        b.update(inner)\n",
        "        self.bigram['_UNK'] = b\n",
        "      else:\n",
        "        self.bigram[word] = inner\n",
        "\n",
        "    for word, inner in list(self.trigram.items()):\n",
        "      for word2, freq in list(inner.items()):\n",
        "        if self.unigram.get(word2, 0) == 0:\n",
        "          inner['_UNK'] = inner.get('_UNK', 0) + freq\n",
        "          del inner[word2]\n",
        "      \n",
        "      if self.unigram.get(word, 0) == 0:\n",
        "        del self.trigram[word]\n",
        "        b = self.trigram.get('_UNK', {})\n",
        "        b.update(inner)\n",
        "        self.trigram['_UNK'] = b\n",
        "      else:\n",
        "        self.trigram[word] = inner\n",
        "\n",
        "  def _discount(self, d):\n",
        "    self.kn = {}\n",
        "\n",
        "    for word1 in self.bigram:\n",
        "      for word2 in self.bigram[word1]:\n",
        "        self.bigram[word1][word2] -= d\n",
        "        self.kn[word2] = self.kn.get(word2, 0) + 1\n",
        "      self.bigram[word1]['_DISCOUNT'] = d * len(self.bigram[word1])\n",
        "\n",
        "    for word1 in self.trigram:\n",
        "      for word2 in self.trigram[word1]:\n",
        "        self.trigram[word1][word2] -= d\n",
        "      self.trigram[word1]['_DISCOUNT'] = d * len(self.trigram[word1])\n",
        "\n",
        "  def generate_sentence(self, n=None, method='unigram', k=10):\n",
        "    blacklist = ['_START', '_UNK', '_DISCOUNT']\n",
        "    if method == 'unigram':\n",
        "      words = {k:v for k,v in self.unigram.items() if k not in blacklist+['_END']}\n",
        "      top_k = sorted(words, key=words.get, reverse=True)[:k]\n",
        "      sentence = [random.choice(top_k)]\n",
        "\n",
        "      if n == None:\n",
        "        print('Parameter \\'n\\' must be provided when generating sentence from the unigram model.')\n",
        "        return\n",
        "\n",
        "      while len(sentence) < n:\n",
        "        words = {k:v for k,v in self.unigram.items() if k not in blacklist+['_END']}\n",
        "        top_k = sorted(words, key=words.get, reverse=True)[:k]\n",
        "        sentence.append(random.choice(top_k))\n",
        "        \n",
        "      return ' '.join(sentence)\n",
        "    elif method == 'bigram' or method == 'trigram':\n",
        "      source = self.bigram if method == 'bigram' else self.trigram\n",
        "      words = {k:v for k,v in source['_START'].items() if k not in blacklist}\n",
        "      top_k = sorted(words, key=words.get, reverse=True)[:k]\n",
        "      sentence = [random.choice(top_k)]\n",
        "\n",
        "      if n == None:\n",
        "        while sentence[-1] != '_END':\n",
        "          words = {k:v for k,v in source[sentence[-1]].items() if k not in blacklist}\n",
        "          top_k = sorted(words, key=words.get, reverse=True)[:k]\n",
        "          sentence.append(random.choice(top_k))\n",
        "\n",
        "        sentence.pop()\n",
        "      else:\n",
        "        while len(sentence) < n:\n",
        "          words = {k:v for k,v in source[sentence[-1]].items() if k not in blacklist+['_END']}\n",
        "          top_k = sorted(words, key=words.get, reverse=True)[:k]\n",
        "          sentence.append(random.choice(top_k))\n",
        "      return ' '.join(sentence)\n",
        "    else:\n",
        "      print('Not implemented: {}'.format(method)) \n",
        "      return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIfpuAPNlfpQ"
      },
      "source": [
        "trainingfiles,heldoutfiles=get_training_testing(split=0.8,amount=5)\n",
        "mylm = language_model(files=trainingfiles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adAa6CjBwfwL"
      },
      "source": [
        "mylm.generate_sentence(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzc5sR-Iwfze"
      },
      "source": [
        "mylm.generate_sentence(method='bigram')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8BOcmi6weUT"
      },
      "source": [
        "mylm.generate_sentence(10, method='bigram')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvo1BkXuw3w-"
      },
      "source": [
        "mylm.generate_sentence(method='trigram')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "656_ETliw30p"
      },
      "source": [
        "mylm.generate_sentence(10, method='trigram')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIwVpPQex0R1"
      },
      "source": [
        "mylm.generate_sentence(method='bigram', k=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdW22X0Yxf9a"
      },
      "source": [
        "mylm.generate_sentence(method='trigram', k=20)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
